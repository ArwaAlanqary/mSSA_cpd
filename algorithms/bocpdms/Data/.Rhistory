actual_cutoffs <- seq(0.4, 0.7, 0.1)
}else if(mode == "autocorr"){
actual_cutoffs <- seq(0.05, 0.20, 0.05)
}
cutoffs = c(0,actual_cutoffs,0.99)
l = length(actual_cutoffs)
#get an idea of how many stations are in which nbhs
if(show_histograms){
nbh_sizes = matrix(0, nrow = 30, ncol = l)
for(i in 1:l){
nbh_sizes[,i] <- rowSums( (cutoffs[i]<correlations & correlations <=cutoffs[i+1] ) )
}
#histogram shows how large the nbhs are for that particular cuttoff
for(i in 1:l){
hist(nbh_sizes[,i], breaks = seq(0,30,1))
}
}
#Form nbhs based on how correlated the portfolios are, using the cutoffs determined above
#in line 17 as 'actual_cutoffs'
nbh_lists = list()
for(i in 1:l){
mat <- (cutoffs[i]<correlations & correlations <=cutoffs[i+1])
#DEBUG: Make sure we do not select autocorr of own TS into nbhs
diag(mat) = FALSE
nbh_lists[[i]] <- mat
}
nbh_lists_of_nbh_lists[[decade]] <- nbh_lists
#To get a feel for how many parameters we induce, calculate it by forming the nbhs
#indices = 1:30
#induced_nbhs = list()
#for(i in 1:l){
#  induced_nbhs[[i]] = list()
#  for(j in 1:30){
#    ith_nbh_for_j = indices[nbh_lists[[i]][j,]]
#    induced_nbhs[[i]][[j]] = ith_nbh_for_j
#  }
#}
}
#count the #of params
#num_params = list()
#for(i in 1:l){
##count_total = 0
#  count_i = 0
#  for(j in 1:30){
#    count_i = count_i + length(induced_nbhs[[i]][[j]])
#  }
#  num_params[[i]] = count_i
#  count_total = count_total + num_params[[i]]
#}
#print(num_params)
#print(count_total)
#count up #params for different nbh sizes
#all_pars = 1:l
#pars = 0
#for(i in l:1){
#  pars = pars+num_params[[i]]
#  all_pars[i] = pars
#}
#print(all_pars)
#Let us get the autocorrelation of each series to check HOW good a predictor the other
#correlated series are (i.e., if my portolio can already be well predicted by its own past, there
#is relatively little need to use low-correlated other portfolios for prediction)
#T_ = length(data[,1])
#autocorrelations <- diag(cor(data[-T_,], data[2:T_,]))
#Note: Not really helpful because autocorrelations much smaller than correlations
#Lastly, save the l groups into l files containing 30x30 matrices IF you think they
#constitute a good grouping
if(grouping_accepted){
write.csv(cutoffs, paste0(mode, "/portfolio_grouping_cutoffs.csv"), row.names=F)
for(decade in 1:num_decades){
#if needed, create new folder
if(!dir.exists(paste0(getwd(), "/", mode, "/decade_", decade))){
dir.create(paste0(getwd(), "/", mode,  "/decade_", decade))
}
for(i in 1:l){
write.csv(nbh_lists_of_nbh_lists[[decade]][[i]], paste0(mode, "/decade_", decade,
"/portfolio_grouping_", i, ".csv"), row.names=F)
}
}
}
##### Preamble ######
library(osrm)
setwd(paste0("/Users/jeremiasknoblauch/Documents/Studieren - Inhaltliches/",
"OxWaSP PC backup/Modules/SpatialProject/Code/SpatialBOCD/Data/AirPollutionData/CongestionChargeData"))
###stations for which we need distances###
stationIDs = c("BL0", "BT1", "BX1", "CR2", "GR4", "GR5", "HI1", "HS4", "HV1", "HV3", "KC1", "LW1", "RB3", "RB4", "TH1")
stationIDs = as.factor(stationIDs)
###Read in the lon, lat, and ids###
rawdataset = read.csv(paste0("/Users/jeremiasknoblauch/Documents/Studieren - Inhaltliches/",
"OxWaSP PC backup/Modules/SpatialProject/Code/SpatialBOCD/Data", "/sites_range_hourly.csv"))
lon_lat = matrix(0, nrow=length(stationIDs), ncol=2)
count = 1
for(id in stationIDs){
first_entry= (rawdataset[which(rawdataset[,1] == id),])[1,]
lon_lat[count,1] = first_entry[1,3]
lon_lat[count,2] = first_entry[1,4]
count = count+1
}
###Put it together in data frame###
srcdest = data.frame(stationIDs, lon_lat[,1], lon_lat[,2])
#### Find the distance along a road
colnames(srcdest)<-c("UID","Lon","Lat") # rename the columns to be meaningful
DistanceMatrix = matrix(0, nrow=length(stationIDs), ncol=length(stationIDs))
for (i in 1:nrow(srcdest)){ #set up a loop so that we can build a matrix between source and destination for all points in the input table
#get an empty row of distances
row = vector(length= length(stationIDs))
for (j in 1:nrow(srcdest)){
DistanceMatrixi <- osrmRoute(src=srcdest[i, c("UID","Lon","Lat")],dst=srcdest[j, c("UID","Lon","Lat")],sp=TRUE) # produces one value of distance_i,j # You must input the source location and destination location such that it shouws an ID, longitude and latitude, # must be in WGS84
Single<-DistanceMatrixi$distance # selects the output that you are interested in (you can pick distance or duration)
#Appended<-cbind(Appended, Single) #goes through every iteration of j to find the distance between a single value i and all values j
row[j] = Single
}
DistanceMatrix[i,] = row
paste("This has looped",print(i))
}
### Average (i,j) and (j,i) entries ###
SymmetricDistanceMatrix = matrix(0, nrow=length(stationIDs), ncol=length(stationIDs))
for (i in 1:nrow(srcdest)){
for (j in 1:nrow(srcdest)){
if(i>j){
SymmetricDistanceMatrix[i,j] = 0.5*(DistanceMatrix[i,j] + DistanceMatrix[j,i])
SymmetricDistanceMatrix[j,i] = 0.5*(DistanceMatrix[i,j] + DistanceMatrix[j,i])
}
}
}
# The output should be a symmetric matrix given that you are testing all points to all other points in the same dataset. If you have different input files for source and distination then this will not be symmetric.
write.table(SymmetricDistanceMatrix, paste0("/Users/jeremiasknoblauch/Documents/Studieren - Inhaltliches/",
"OxWaSP PC backup/Modules/SpatialProject/Code/SpatialBOCD/Data/AirPollutionData/CongestionChargeData",
"/RoadDistanceMatrix.csv"), append = FALSE, sep=",", row.names = FALSE, col.names = FALSE)
###                                     ###
###     DISTANCES IN EUCLIDEAN          ###
###                                     ###
###                                     ###
# Calculate distance in kilometers between two points
earth.dist <- function (long1, lat1, long2, lat2)
{
rad <- pi/180
a1 <- lat1 * rad
a2 <- long1 * rad
b1 <- lat2 * rad
b2 <- long2 * rad
dlon <- b2 - a2
dlat <- b1 - a1
a <- (sin(dlat/2))^2 + cos(a1) * cos(b1) * (sin(dlon/2))^2
c <- 2 * atan2(sqrt(a), sqrt(1 - a))
R <- 6378.145
d <- R * c
return(d)
}
# do it on data set
#get IDs and the lon, lat
IDs = stationIDs #union(data[,1], data[,1])
#station_properties = read.csv(paste0(folder_dir, "/stations.txt"), sep=",")
#relevant_stations = station_properties[station_properties[,1] %in% IDs,]
#lon, lat are factors > convert into numerics!
lons = srcdest$Lon
lats = srcdest$Lat
num_stat = length(IDs)
#distances will be in kilometer
pairwise_dist = matrix(0, nrow=num_stat, ncol=num_stat)
for(i in 1:num_stat){
for(j in 1:num_stat){
lon1 = convert_to_decimal(lons[i])
lon2 = convert_to_decimal(lons[j])
lat1 = convert_to_decimal(lats[i])
lat2 = convert_to_decimal(lats[j])
pairwise_dist[i,j] = earth.dist(lon1,lat1,lon2,lat2)
}
}
pairwise_dist
lon1
lons
pairwise_dist = matrix(0, nrow=num_stat, ncol=num_stat)
for(i in 1:num_stat){
for(j in 1:num_stat){
lon1 = (lons[i])
lon2 = (lons[j])
lat1 = (lats[i])
lat2 = (lats[j])
pairwise_dist[i,j] = earth.dist(lon1,lat1,lon2,lat2)
}
}
pairwise_dist
colnames(pairwise_dist) = IDs
#save file
write.table(pairwise_dist,
paste0("/Users/jeremiasknoblauch/Documents/Studieren - Inhaltliches/",
"OxWaSP PC backup/Modules/SpatialProject/Code/SpatialBOCD/Data/AirPollutionData/CongestionChargeData",
"/EuclideanDistanceMatrix.csv"), append = FALSE, sep=",", row.names = FALSE, col.names = FALSE)
pairwise_dist
SymmetricDistanceMatrix
hist(SymmetricDistanceMatrix)
hist(pw_distances)
hist(pw_dist)
hist(pairwise_dist)
?write.table
#We have cycle length of 365*24*4 = 35040
#1-hour segment length is 4*1 = 4
#4-hour segment length is 4*4 = 16
#day length is 24*4 = 96
#week length is 24*4*7 = 672
#30-day (month) length is 35040/12 = 2920
#year length is 35040
#Idea: Year-cycle, week-cycle, day-cycle
#       i.e., m_d = 96, m_w = 672, m_y = 35040 and then use
#       k=1 for each m.
#Equivalently: m = m_y and k = {m_y/m_d, m_y/m_w, m_y/m_y}
setwd(paste0("/Users/jeremiasknoblauch/Documents/Studieren - Inhaltliches/OxWaSP PC backup",
"/Modules/SpatialProject/Code/SpatialBOCD/Data"))
rawData <- read.csv(file="sites_range_minute.csv", header=TRUE, sep=",")
#Modify the date time entries s.t. we get sequence from 1 through to T (while retaining year, month)
rawData <- rawData[,-2]
rawData <- rawData[,-2]
#Chop off the dates beyond our scope:
#latest start date: 1999, 21.11. at 0:00
#earliest stop date: 2010, 27.08. at 23:45
rawData <- rawData[which(rawData$year>1999 | (rawData$year == 1999 & rawData$month>=11 & rawData$day>=22)),]
rawData <- rawData[which(rawData$year<2010 | (rawData$year == 2010 & rawData$month<=8 & rawData$day<=27)),]
#Format the data: (1) Get the ids for the stations, (2) convert into a useful date format: Days i.e. 365*year +
stationIDs = attributes(rawData[,1])$levels
num_stations = length(stationIDs)
all_ranges_numeric = matrix(0, nrow=num_stations, ncol=2*5)
#loop over all the stations. For each, extract the data and convert data format.
row_count = 1
for(id in 1:num_stations){
#Show id of current station
print(id)
#Retrieve all data for this station
stationData = rawData[which(rawData[,1] == stationIDs[id]),]
all_data_points = length(stationData[,1])
#Get minimum and maximum dates as strings
#all_ranges_string[id,2] = as.POSIXlt(stationData$datetime[1])
#all_ranges_string[id,1] = as.POSIXlt(stationData$datetime[length(stationData[,1])])
#Get minimum and maximum dates as numerics
all_ranges_numeric[id,1] = stationData$year[1]
all_ranges_numeric[id,2] = stationData$month[1]
all_ranges_numeric[id,3] = stationData$day[1]
all_ranges_numeric[id,4] = stationData$hour[1]
all_ranges_numeric[id,5] = stationData$minute[1]
all_ranges_numeric[id,6] = stationData$year[all_data_points]
all_ranges_numeric[id,7] = stationData$month[all_data_points]
all_ranges_numeric[id,8] = stationData$day[all_data_points]
all_ranges_numeric[id,9] = stationData$hour[all_data_points]
all_ranges_numeric[id,10] = stationData$minute[all_data_points]
#Get generic time stamps
time_stamps = 1:length(stationData[,1])
}
#Check if the ranges are all the same
print(all_ranges_numeric)
for(id in 1:num_stations){
#Select the relevant entries
stationData <- rawData[which(rawData[,1] == stationIDs[id]),]
#insert the time stamp and change the name
stationData$site <- time_stamps
colnames(stationData) <- c("timestamp", colnames(stationData)[-1])
#Save into file <ID>.txt
#write.table(stationData, file = paste0(stationIDs[id], ".txt") )
#pdf("ST3_data.pdf")
plot(ts(stationData$value, start = c(1999, 11), frequency = 2920*12))
#dev.off()
}
#Save the result
#latest start date: 1999, 21.11. at 0:00
#earliest stop date: 2010, 27.08. at 23:45
#NOTE: station with 6th ID ( = CD1) should be excluded/needs to be checked (seems to miss data)
#       CD1 can be trimmed (first all_data_points/8 points can be chopped off)
#Find out which range is the largest where we have no missings at all
#Way to do this: Extract a boolean vector per station and then take the conjunction
#Before you do that, visually inspect stations that have too many missings already anyways: 1, 2, 6, 8?[two minor gaps],
# 9? [similar to 8], 12 [ only starts mid-2000], 14, 17? [two small gaps], 18? [three small/medium gaps],
#22? [two/three small/medium gaps], 24, 28? [has major gaps from halfway 2007 onwards], 30 [medium gap before 2002, major gap at 2007],
#33, 38? [multiple small and one medium gap]
#Get missings
excluding_strict = c(1,2,6,8,9,12,14,17,18,22,24,28,30,33,38)
excluding_lenient = c(1,2,6,14,24,30,33)
exclusion = excluding_strict
missing_vector = matrix(0, ncol=num_stations - length(exclusion), nrow = length(stationData$value))
count = 1
for(id in setdiff((1:num_stations), exclusion)){
#Select the relevant entries
stationData <- rawData[which(rawData[,1] == stationIDs[id]),]
#check which ones are missing
missing_vector[,count] = is.na(stationData$value)
count = count+1
}
time_stamps = 1:length(stationData[,1])
#Plot the missings all in one plot s.t. we insert NA for all 0s (i.e., we get no color in the plot
#for entries that are not missing)
#The missing for station with id i will all be printed at level i.
missing_vector_weighted = t( t( missing_vector ) *  setdiff((1:num_stations), exclusion) )
plot(ts(missing_vector_weighted[,1],start = c(1999,11), frequency = 2920*12), type="p")
#Plot IF there is a missing at t, i.e. take the row sums of missing_vector
any_missing = rowSums(missing_vector) > 0
#plot(ts(any_missing, start = c(1999,11), frequency = 2920*12))
plot(any_missing, type = "p")
#Next: daily averages (where we make sure that NAs are not taken into account for these averages) +
#check for NAs again
#STEP 1: Transform raw data into daily averages
num_obs_all = length(rawData$value)
num_days = num_obs_all/96
rawData_day_values = matrix(0, nrow=num_days, ncol = 1)
for(i in 1:num_days){
rawData_day_values[i,1] = mean(rawData$value[(96*(i-1)+1):(96*(i))],na.rm=TRUE)
}
rawData_days = rawData[96*(1:num_days),]
rawData_days$value = rawData_day_values
rawData_days = rawData_days[,-5]
rawData_days = rawData_days[,-5]
#STEP 2: Plot
for(id in 1:num_stations){
#Select the relevant entries
stationData <- rawData_days[which(rawData_days[,1] == stationIDs[id]),]
#insert the time stamp and change the name
stationData$site <- 1:(num_days/38)
colnames(stationData) <- c("timestamp", colnames(stationData)[-1])
#Save into file <ID>.txt
write.table(stationData, file = paste0(stationIDs[id], "_daily.txt") )
#pdf("ST3_data.pdf")
plot(ts(stationData$value, start = c(1999, 11), frequency = 365))
#dev.off()
}
#STEP 3: Find out how bad missingness is for daily series
excluding_strict = c(1,2,6,8,9,12,14,17,18,22,24,28,30,33,38)
excluding_lenient = c(1,2,6,14,24,30,33)
exclusion = excluding_strict
missing_vector = matrix(0, ncol=num_stations - length(exclusion), nrow = length(stationData$value))
count = 1
for(id in setdiff((1:num_stations), exclusion)){
#Select the relevant entries
stationData <- rawData_days[which(rawData_days[,1] == stationIDs[id]),]
#check which ones are missing
missing_vector[,count] = is.na(stationData$value)
count = count+1
}
#Plot IF there is a missing at t, i.e. take the row sums of missing_vector
any_missing = rowSums(missing_vector[,1]) > 0
#plot(ts(any_missing, start = c(1999,11), frequency = 2920*12))
plot(missing_vector[,1], type="p")
threshold = 49
consec_missings = c()
for(id in 1:ncol(missing_vector)){
#plot(missing_vector[,id], type="p")
print(id)
stat_id = setdiff((1:num_stations), exclusion)[id]
print(stat_id)
print(sum(missing_vector[,id]))
consecs = c()
maximum = 0
last = 0
for(j in 1:length(missing_vector[,1])){
if(missing_vector[j,id]>0){
last = last+1
maximum = max(maximum, last)
}else{
last = 0
}
if(last > threshold && (j == length(missing_vector[,1]) || missing_vector[j+1,id] == 0) ){
print(paste("We have exceeded", threshold, "at entry", j-last+1, "with a total of",
last, "consecutive missings"))
consecs = c(consecs, c(stat_id, j-last+1, last))
}
}
#don't add consecs if they have no entries
if(!is.null(consecs)){
consec_missings = cbind(consec_missings, matrix(consecs, nrow=3))
}
print(paste("Maximum number of consecutive missings is", maximum))
}
all_troublers = union(consec_missings[1,], consec_missings[1,])
all_stats = setdiff((1:num_stations), exclusion)
no_troublers = setdiff(all_stats, all_troublers)
print(no_troublers)
#plot(any_missing, type = "p")
empty = matrix(0, nrow=length(stationData$value), ncol=1)
empty[consec_missings[2,],1] = consec_missings[1,]
plot(empty)
#MAX: 7 WEEKS (after around 1200, no more such lengthy gaps)
t = 1000
for(i in 1:ceiling(length(empty)/t)){
plot(empty[(t*(i-1)+1):(t*i)])
}
#TO DO:
# (0) fill in missings
# (1) deseasonalized version
# (2) road distance computation + matching IDs to lon, lat coordinates
##
#Check years around 17/02/2003 and 02/2007 [6 months before and after]
congestionCharge <- rawData[which(rawData$year>2002 | (rawData$year == 2002 & rawData$month>=8 & rawData$day>=17)),]
congestionCharge <- congestionCharge[which((rawData$year == 2003 & rawData$month<=8 & rawData$day<=17)),]
#exclusion = c(3,)
exclusion = c()
new_exclusions = c()
for(id in setdiff((1:num_stations), exclusion)){
#Select the relevant entries
stationData <- congestionCharge[which(congestionCharge[,1] == stationIDs[id]),]
#check which ones are missing
if(sum(is.na(stationData$value)) < length(stationData$value)){
plot(ts(stationData$value, start=c(2002, 8), frequency = 2920*12), ylab = id)
}else{
new_exclusions = c(new_exclusions, id)
}
#count = count+1
}
gaps = c(2,6,10,11,16,17,19,20,21,26,27,34,36)
all_excluded = c(gaps, new_exclusions)
#some missings to fill: 6,16, 26,
missing_counts = matrix(0, nrow = length(setdiff((1:num_stations), all_excluded)), ncol=1)
count=1
for(id in setdiff((1:num_stations), all_excluded)){
stationData <- congestionCharge[which(congestionCharge[,1] == stationIDs[id]),]
missing_counts[count] = sum(is.na(stationData$value))
count = count+1
}
for(id in setdiff((1:num_stations), all_excluded)){
stationData <- congestionCharge[which(congestionCharge[,1] == stationIDs[id]),]
plot(ts(stationData$value))
}
#fill in data of each station average between two most adjacent observations, store daily data
for(id in setdiff((1:num_stations), all_excluded)){
stationData <- congestionCharge[which(congestionCharge[,1] == stationIDs[id]),]
T_ = length(stationData$value)
missing_indices = which(is.na(stationData$value))
#fill in by averaging next LHS and RHS calue
for(index in missing_indices){
#search for first non-missing value on LHS
non_missing_LHS = index-1
found_LHS = FALSE
while(non_missing_LHS>0 && !found_LHS){
if(!is.na(stationData$value[non_missing_LHS])){
found_LHS = TRUE
}else{
non_missing_LHS = non_missing_LHS-1
}
}
#search for first non-missing value on RHS
non_missing_RHS = index + 1
found_RHS = FALSE
while(non_missing_RHS<(T_+1) && !found_RHS){
if(!is.na(stationData$value[non_missing_RHS])){
found_RHS = TRUE
}else{
non_missing_RHS = non_missing_RHS+1
}
}
#average them
if(found_RHS && found_LHS){
stationData$value[index] = 0.5*(stationData$value[non_missing_RHS] +
stationData$value[non_missing_LHS])
}else if(found_RHS){
stationData$value[index] = stationData$value[non_missing_RHS]
}else{
stationData$value[index] = stationData$value[non_missing_LHS]
}
}
#plot filled in TS
#plot(ts(stationData$value), ylab = id)
write.table(stationData$value, file = paste0("AirPollutionData/CongestionChargeData/",stationIDs[id],
"_081702-081703_daily.txt"),row.names=FALSE )
#save filled in TS
}
all_excluded = c(all_excluded, 8, 30)
#fill in data of each station average between two most adjacent observations, store 3h data
for(id in setdiff((1:num_stations), all_excluded)){
stationData <- congestionCharge[which(congestionCharge[,1] == stationIDs[id]),]
T_ = length(stationData$value)
missing_indices = which(is.na(stationData$value))
#fill in by averaging next LHS and RHS calue
for(index in missing_indices){
#search for first non-missing value on LHS
non_missing_LHS = index-1
found_LHS = FALSE
while(non_missing_LHS>0 && !found_LHS){
if(!is.na(stationData$value[non_missing_LHS])){
found_LHS = TRUE
}else{
non_missing_LHS = non_missing_LHS-1
}
}
#search for first non-missing value on RHS
non_missing_RHS = index + 1
found_RHS = FALSE
while(non_missing_RHS<(T_+1) && !found_RHS){
if(!is.na(stationData$value[non_missing_RHS])){
found_RHS = TRUE
}else{
non_missing_RHS = non_missing_RHS+1
}
}
#average them
if(found_RHS && found_LHS){
stationData$value[index] = 0.5*(stationData$value[non_missing_RHS] +
stationData$value[non_missing_LHS])
}else if(found_RHS){
stationData$value[index] = stationData$value[non_missing_RHS]
}else{
stationData$value[index] = stationData$value[non_missing_LHS]
}
}
#average every 3h of time, 4*3 = 12 measurements
T_daily = length(stationData$value)
stat_dat = colMeans(matrix(stationData$value, nrow=8, ncol=T_daily/8))
plot(ts(stat_dat), ylab = id)
write.table(stat_dat, file = paste0("AirPollutionData/CongestionChargeData/",stationIDs[id], "_081702-081703_2h.txt"),
row.names=FALSE)
#save filled in TS
}
30*252
25*7600/3600
1.96*0.0523
1.96*0.0448
1.96*0.0139
1.96*0.0126
